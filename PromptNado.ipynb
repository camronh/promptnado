{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-anthropic langsmith python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_system_prompt = \"\"\"You are a helpful assistant\n",
    "\n",
    "Rules:\n",
    "- You are only allowed to talk about coding\n",
    "- <HERE>\n",
    "- Try to be concise\"\"\"\n",
    "\n",
    "example_instruction = \"The agent should only respond in English. No other languages\"\n",
    "\n",
    "examples = [\"Hola, como estas?\", \"Hi there!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith import evaluate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Schemas\n",
    "\n",
    "\n",
    "class Rule(BaseModel):\n",
    "    \"\"\"A single rule for the prompt\"\"\"\n",
    "    reasoning: str = Field(\n",
    "        ..., description=\"The thought process and direction for why we think this is a good solution to the instruction.\")\n",
    "    prompt: str = Field(..., description=\"A single prompt rule that we can try to solve for the instruction. \\\n",
    "This prompt rule will be interpolated into the system prompt over the <HERE> token.\")\n",
    "\n",
    "\n",
    "class Rules(BaseModel):\n",
    "    \"\"\"Set of prompt rules that should be tried\"\"\"\n",
    "    rules: List[Rule] = Field(\n",
    "        ..., description=\"A list of prompt rules that we can try to solve for the instruction.\")\n",
    "\n",
    "\n",
    "class CorrectnessEvaluationResult(BaseModel):\n",
    "    \"\"\"Result of an evaluation of correctness\"\"\"\n",
    "    reasoning: str = Field(\n",
    "        ..., description=\"The thought process behind why you think the answer is correct or incorrect.\")\n",
    "    correct: bool = Field(..., description=\"Correctness score\")\n",
    "\n",
    "\n",
    "class Promptnado:\n",
    "    def __init__(self, system_prompt: str, instruction: str, examples: List[str], rule_token=\"<HERE>\"):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.instruction = instruction\n",
    "        self.examples = examples\n",
    "        self.rule_token = rule_token\n",
    "\n",
    "        # Create random dataset name\n",
    "        self.dataset_name = f\"Promptnado_{random.randint(0, 1000000)}\"\n",
    "\n",
    "        self.attempts = 1\n",
    "        self.solved = False\n",
    "        self.current_rule = None\n",
    "        self.current_prompt = None\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        \"\"\"Create a dataset with a unique name\"\"\"\n",
    "        dataset = client.create_dataset(\n",
    "            self.dataset_name, description=self.instruction)\n",
    "        for example in self.examples:\n",
    "            client.create_example(\n",
    "                inputs={\"input\": example}, dataset_id=dataset.id)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        print(\n",
    "            f\"Created dataset: {self.dataset_name} with {len(self.examples)} examples\")\n",
    "        return dataset\n",
    "\n",
    "    def _generate_rules(self):\n",
    "        \"\"\"Use an LLM to generate a list of rules\"\"\"\n",
    "\n",
    "        system_prompt = f\"\"\"You are an expert LLM Prompt Engineer. Your job is to try to solve for the provided <Instructions> \\\n",
    "by making adjustments to the <Original Prompt>. You should attempt to make 5 suggestions for prompts that might work. Each suggestion you \\\n",
    "make will be interpolated into the prompt where {self.rule_token} is, and then evaluated for correctness against a dataset of \\\n",
    "examples.\n",
    "\n",
    "<Instructions>\n",
    "{self.instruction}\n",
    "</Instructions>\n",
    "\n",
    "<Original Prompt>\n",
    "{self.system_prompt}\n",
    "</Original Prompt>\n",
    "\"\"\"\n",
    "\n",
    "        structured_llm = init_chat_model(\n",
    "            model=\"gpt-4o\", temperature=0.7).with_structured_output(Rules)\n",
    "\n",
    "        rules: Rules = structured_llm.invoke(system_prompt)\n",
    "\n",
    "        self.rules = rules.rules\n",
    "        print(f\"Generated {len(self.rules)} rules\\n\")\n",
    "        print(self.rules)\n",
    "        return self.rules\n",
    "\n",
    "    def _build_prompt(self, rule: Rule):\n",
    "        \"\"\"Interpolate the rules into the system prompt\"\"\"\n",
    "        interpolated_prompt = self.system_prompt.replace(\n",
    "            self.rule_token, rule.prompt)\n",
    "\n",
    "        print(f\"Interpolated prompt:\\n\\n{interpolated_prompt}\")\n",
    "        return interpolated_prompt\n",
    "\n",
    "    def _evaluate_correctness(self, run: Run, example: Example):\n",
    "        \"\"\"Eval function to use an LLM to validate that the instruction was followed\"\"\"\n",
    "        system_prompt = f\"\"\"Your job is to validate whether the <Result> meets the criteria for <Instruction>. Try to be a harsh judge.\n",
    "\n",
    "<Instruction>\n",
    "{self.instruction}\n",
    "</Instruction>\n",
    "\n",
    "<Result>\n",
    "{run.outputs[\"output\"]}\n",
    "</Result>\n",
    "\"\"\"\n",
    "\n",
    "        structured_llm = init_chat_model(\n",
    "            model=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessEvaluationResult)\n",
    "\n",
    "        result: CorrectnessEvaluationResult = structured_llm.invoke(\n",
    "            system_prompt)\n",
    "\n",
    "        return {\"score\": 1 if result.correct else 0, \"key\": \"correctness\", \"comment\": result.reasoning}\n",
    "\n",
    "    def _predict(self, inputs: dict):\n",
    "        \"\"\"Run current prompt against example in the dataset\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.current_prompt),\n",
    "            HumanMessage(content=inputs[\"input\"]),\n",
    "        ]\n",
    "\n",
    "        # Invoke the model\n",
    "        llm = init_chat_model(model=\"gpt-4o\", temperature=0.7)\n",
    "        response = llm.invoke(messages)\n",
    "\n",
    "        return {\"output\": response.content}\n",
    "\n",
    "    def _test_rule(self, rule: Rule):\n",
    "        \"\"\"Evaluate a given rule\"\"\"\n",
    "        print(f\"\\nTesting rule: {rule.prompt}\")\n",
    "        self.current_rule = rule\n",
    "\n",
    "        self.current_prompt = self._build_prompt(self.current_rule)\n",
    "\n",
    "        results = evaluate(\n",
    "            self._predict,\n",
    "            data=self.dataset_name,\n",
    "            evaluators=[self._evaluate_correctness],\n",
    "            experiment_prefix=f\"Attempt-{self.attempts}\",\n",
    "        )\n",
    "\n",
    "        self.attempts += 1\n",
    "\n",
    "        return results\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the promptnado\"\"\"\n",
    "        print(f\"Running Promptnado with instruction: {self.instruction}\")\n",
    "        # Create the dataset\n",
    "        self._create_dataset()\n",
    "\n",
    "        while self.solved == False:\n",
    "            # Get a list of rules\n",
    "            self._generate_rules()\n",
    "\n",
    "            # For each rule\n",
    "            for rule in self.rules:\n",
    "                results = self._test_rule(rule)\n",
    "\n",
    "                # Add results validation here\n",
    "\n",
    "                self.results = results\n",
    "                self.solved = True\n",
    "                break\n",
    "\n",
    "        print(\"\\n\\nSolved!! Current prompt can be found at `self.current_prompt\\n\\n\")\n",
    "\n",
    "        print(f\"Current prompt:\\n\\n{self.current_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Promptnado with instruction: The agent should only respond in English. No other languages\n",
      "Created dataset: Promptnado_863457 with 2 examples\n",
      "Generated 5 rules\n",
      "\n",
      "[Rule(reasoning='This rule explicitly instructs the LLM to respond only in English, ensuring that no other languages are used.', prompt='You must only respond in English. No other languages are allowed.'), Rule(reasoning='By specifying the language explicitly, it reinforces the instruction to not deviate from English in responses.', prompt='All your responses must be in English only.'), Rule(reasoning='A clear directive that emphasizes the exclusivity of using English in all responses.', prompt='You are required to communicate exclusively in English.'), Rule(reasoning='This rule adds a constraint to use only English, making it clear that responses in other languages are not acceptable.', prompt='Respond only in English. Use of other languages is prohibited.'), Rule(reasoning='A straightforward rule that makes it clear that responses must be in English only, ensuring compliance with the instruction.', prompt='English is the only language you are allowed to use in your responses.')]\n",
      "\n",
      "Testing rule: You must only respond in English. No other languages are allowed.\n",
      "Interpolated prompt:\n",
      "\n",
      "You are a helpful assistant\n",
      "\n",
      "Rules:\n",
      "- You are only allowed to talk about coding\n",
      "- You must only respond in English. No other languages are allowed.\n",
      "- Try to be concise\n",
      "View the evaluation results for experiment: 'Attempt-1-a8c6b7ce' at:\n",
      "https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/datasets/5a9f12dc-5b38-4f25-9717-f49653f2b1ed/compare?selectedSessions=18191015-9ccc-4e38-b8dd-d06c8b0f056a\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solved!! Current prompt can be found at `self.current_prompt\n",
      "\n",
      "\n",
      "Current prompt:\n",
      "\n",
      "You are a helpful assistant\n",
      "\n",
      "Rules:\n",
      "- You are only allowed to talk about coding\n",
      "- You must only respond in English. No other languages are allowed.\n",
      "- Try to be concise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pn = Promptnado(example_system_prompt, example_instruction, examples)\n",
    "pn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'run': RunTree(id=UUID('bcc2dcf8-ab91-41a2-952c-6f406ca5ce95'), name='Target', start_time=datetime.datetime(2024, 7, 17, 21, 48, 34, 437668, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 7, 17, 21, 48, 35, 457420, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'b3f9496-dirty', 'num_repetitions': 1, 'example_version': '2024-07-17T21:48:29.706692+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.89', 'library': 'langsmith', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.1', 'langchain_version': '0.2.9', 'langchain_core_version': '0.2.20', 'thread_count': 21.0, 'mem': {'rss': 145113088.0}, 'cpu': {'time': {'sys': 0.917068864, 'user': 4.751277056}, 'ctx_switches': {'voluntary': 37312.0, 'involuntary': 0.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': '(inputs: dict)', 'doc': 'Run current prompt against example in the dataset'}, events=[], inputs={'inputs': {'input': 'Hola, como estas?'}}, outputs={'output': 'Hello! How can I assist you with coding today?'}, reference_example_id=UUID('b7773bec-324b-41c9-b76e-ec4965811164'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('f1cce24d-9265-497c-abf8-bd824fe3a12e'), name='ChatOpenAI', start_time=datetime.datetime(2024, 7, 17, 21, 48, 34, 467529, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 7, 17, 21, 48, 35, 456846, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-4o', 'model_name': 'gpt-4o', 'stream': False, 'n': 1, 'temperature': 0.7, 'logprobs': False, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o', 'ls_model_type': 'chat', 'ls_temperature': 0.7}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-4o', 'temperature': 0.7, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 7, 17, 21, 48, 34, 467529, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 7, 17, 21, 48, 35, 456846, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'SystemMessage'], 'kwargs': {'content': 'You are a helpful assistant\\n\\nRules:\\n- You are only allowed to talk about coding\\n- You must only respond in English. No other languages are allowed.\\n- Try to be concise', 'type': 'system'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Hola, como estas?', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'Hello! How can I assist you with coding today?', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Hello! How can I assist you with coding today?', 'response_metadata': {'token_usage': {'completion_tokens': 11, 'prompt_tokens': 53, 'total_tokens': 64}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-f1cce24d-9265-497c-abf8-bd824fe3a12e-0', 'usage_metadata': {'input_tokens': 53, 'output_tokens': 11, 'total_tokens': 64}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 11, 'prompt_tokens': 53, 'total_tokens': 64}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31'}, 'run': None}, reference_example_id=None, parent_run_id=UUID('bcc2dcf8-ab91-41a2-952c-6f406ca5ce95'), tags=[], child_runs=[], trace_id=UUID('bcc2dcf8-ab91-41a2-952c-6f406ca5ce95'), dotted_order='20240717T214834437668Zbcc2dcf8-ab91-41a2-952c-6f406ca5ce95.20240717T214834467529Zf1cce24d-9265-497c-abf8-bd824fe3a12e')], session_name='Attempt-1-a8c6b7ce', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240717T214834437668Zbcc2dcf8-ab91-41a2-952c-6f406ca5ce95', trace_id=UUID('bcc2dcf8-ab91-41a2-952c-6f406ca5ce95')),\n",
       "  'example': Example(dataset_id=UUID('5a9f12dc-5b38-4f25-9717-f49653f2b1ed'), inputs={'input': 'Hola, como estas?'}, outputs=None, metadata={'dataset_split': ['base']}, id=UUID('b7773bec-324b-41c9-b76e-ec4965811164'), created_at=datetime.datetime(2024, 7, 17, 21, 48, 29, 706692, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 7, 17, 21, 48, 29, 706692, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
       "  'evaluation_results': {'results': [EvaluationResult(key='correctness', score=1, value=None, comment='The response is entirely in English and does not contain any words or phrases from other languages. It meets the criteria specified in the instruction.', correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('30cee6cb-d10a-4103-9125-12e0d6eac9b3'), target_run_id=None)]}},\n",
       " {'run': RunTree(id=UUID('f9e0a027-9ab4-4eb3-b98a-eb60d43461ec'), name='Target', start_time=datetime.datetime(2024, 7, 17, 21, 48, 34, 434473, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 7, 17, 21, 48, 35, 461230, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'b3f9496-dirty', 'num_repetitions': 1, 'example_version': '2024-07-17T21:48:29.823377+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.89', 'library': 'langsmith', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.1', 'langchain_version': '0.2.9', 'langchain_core_version': '0.2.20', 'thread_count': 21.0, 'mem': {'rss': 145113088.0}, 'cpu': {'time': {'sys': 0.917068864, 'user': 4.751277056}, 'ctx_switches': {'voluntary': 37312.0, 'involuntary': 0.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': '(inputs: dict)', 'doc': 'Run current prompt against example in the dataset'}, events=[], inputs={'inputs': {'input': 'Hi there!'}}, outputs={'output': 'Hello! How can I assist you with your coding needs today?'}, reference_example_id=UUID('9130ff0b-3e64-4eb8-9ba2-57891aab4021'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('359dec94-7cd2-4753-b92b-9703f57c9d6e'), name='ChatOpenAI', start_time=datetime.datetime(2024, 7, 17, 21, 48, 34, 461570, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 7, 17, 21, 48, 35, 460835, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-4o', 'model_name': 'gpt-4o', 'stream': False, 'n': 1, 'temperature': 0.7, 'logprobs': False, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o', 'ls_model_type': 'chat', 'ls_temperature': 0.7}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-4o', 'temperature': 0.7, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 7, 17, 21, 48, 34, 461570, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 7, 17, 21, 48, 35, 460835, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'SystemMessage'], 'kwargs': {'content': 'You are a helpful assistant\\n\\nRules:\\n- You are only allowed to talk about coding\\n- You must only respond in English. No other languages are allowed.\\n- Try to be concise', 'type': 'system'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Hi there!', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'Hello! How can I assist you with your coding needs today?', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Hello! How can I assist you with your coding needs today?', 'response_metadata': {'token_usage': {'completion_tokens': 13, 'prompt_tokens': 51, 'total_tokens': 64}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-359dec94-7cd2-4753-b92b-9703f57c9d6e-0', 'usage_metadata': {'input_tokens': 51, 'output_tokens': 13, 'total_tokens': 64}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 13, 'prompt_tokens': 51, 'total_tokens': 64}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c4e5b6fa31'}, 'run': None}, reference_example_id=None, parent_run_id=UUID('f9e0a027-9ab4-4eb3-b98a-eb60d43461ec'), tags=[], child_runs=[], trace_id=UUID('f9e0a027-9ab4-4eb3-b98a-eb60d43461ec'), dotted_order='20240717T214834434473Zf9e0a027-9ab4-4eb3-b98a-eb60d43461ec.20240717T214834461570Z359dec94-7cd2-4753-b92b-9703f57c9d6e')], session_name='Attempt-1-a8c6b7ce', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240717T214834434473Zf9e0a027-9ab4-4eb3-b98a-eb60d43461ec', trace_id=UUID('f9e0a027-9ab4-4eb3-b98a-eb60d43461ec')),\n",
       "  'example': Example(dataset_id=UUID('5a9f12dc-5b38-4f25-9717-f49653f2b1ed'), inputs={'input': 'Hi there!'}, outputs=None, metadata={'dataset_split': ['base']}, id=UUID('9130ff0b-3e64-4eb8-9ba2-57891aab4021'), created_at=datetime.datetime(2024, 7, 17, 21, 48, 29, 823377, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 7, 17, 21, 48, 29, 823377, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
       "  'evaluation_results': {'results': [EvaluationResult(key='correctness', score=1, value=None, comment='The response is entirely in English and does not contain any words or phrases from other languages.', correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('601ee0f6-2257-404e-a23e-6ff4a8328fab'), target_run_id=None)]}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.results._results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
