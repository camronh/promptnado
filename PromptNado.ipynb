{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-anthropic langsmith python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_system_prompt = \"\"\"You are a helpful assistant\n",
    "\n",
    "Rules:\n",
    "- You are only allowed to talk about coding\n",
    "- <HERE>\n",
    "- Try to be concise\"\"\"\n",
    "\n",
    "example_instruction = \"The agent should only respond in English. No other languages\"\n",
    "\n",
    "examples = [\"Hola, como estas?\", \"Hi there!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camron/Documents/Dev/promptnado/.venv/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import random\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith import evaluate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "from schemas import Rule, Rules, CorrectnessEvaluationResult\n",
    "\n",
    "\n",
    "class Promptnado:\n",
    "    def __init__(self, system_prompt: str, instruction: str, examples: List[str], rule_token=\"<HERE>\", max_attempts=10,\n",
    "                 rule_gen_model=init_chat_model(\n",
    "                     \"gpt-4o-mini\", temperature=0.7),\n",
    "                 eval_model=init_chat_model(\"gpt-4o-mini\", temperature=0.7),\n",
    "                 prediction_model=init_chat_model(\"gpt-4o-mini\", temperature=0.7)):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.instruction = instruction\n",
    "        self.examples = examples\n",
    "        self.rule_token = rule_token\n",
    "\n",
    "        # Create random dataset name\n",
    "        self.dataset_name = f\"Promptnado_{random.randint(0, 1000000)}\"\n",
    "\n",
    "        self.attempts = 1\n",
    "        self.solved = False\n",
    "        self.current_rule = None\n",
    "        self.current_prompt = None\n",
    "        self.successful_prompt = None\n",
    "        self.rule_gen_model = rule_gen_model\n",
    "        self.eval_model = eval_model\n",
    "        self.prediction_model = prediction_model\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        \"\"\"Create a dataset with a unique name\"\"\"\n",
    "        dataset = client.create_dataset(\n",
    "            self.dataset_name, description=self.instruction)\n",
    "        for example in self.examples:\n",
    "            client.create_example(\n",
    "                inputs={\"input\": example}, dataset_id=dataset.id)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        print(\n",
    "            f\"Created dataset: {self.dataset_name} with {len(self.examples)} examples\")\n",
    "        return dataset\n",
    "\n",
    "    def _generate_rules(self):\n",
    "        \"\"\"Use an LLM to generate a list of rules\"\"\"\n",
    "\n",
    "        system_prompt = f\"\"\"You are an expert LLM Prompt Engineer. Your job is to try to solve for the provided <Instructions> \\\n",
    "by making adjustments to the <Original Prompt>. You should attempt to make 5 suggestions for prompts that might work. Each suggestion you \\\n",
    "make will be interpolated into the prompt where {self.rule_token} is, and then evaluated for correctness against a dataset of \\\n",
    "examples.\n",
    "\n",
    "<Instructions>\n",
    "{self.instruction}\n",
    "</Instructions>\n",
    "\n",
    "<Original Prompt>\n",
    "{self.system_prompt}\n",
    "</Original Prompt>\n",
    "\"\"\"\n",
    "\n",
    "        structured_llm = self.rule_gen_model.with_structured_output(Rules)\n",
    "\n",
    "        rules: Rules = structured_llm.invoke(system_prompt)\n",
    "\n",
    "        self.rules = rules.rules\n",
    "        print(f\"Generated {len(self.rules)} rules\\n\")\n",
    "        print(self.rules)\n",
    "        return self.rules\n",
    "\n",
    "    def _build_prompt(self, rule: Rule):\n",
    "        \"\"\"Interpolate the rules into the system prompt\"\"\"\n",
    "        interpolated_prompt = self.system_prompt.replace(\n",
    "            self.rule_token, rule.prompt)\n",
    "\n",
    "        print(f\"Interpolated prompt:\\n\\n{interpolated_prompt}\")\n",
    "        return interpolated_prompt\n",
    "\n",
    "    def _evaluate_correctness(self, run: Run, example: Example):\n",
    "        \"\"\"Eval function to use an LLM to validate that the instruction was followed\"\"\"\n",
    "        system_prompt = f\"\"\"Your job is to validate whether the <Result> meets the criteria for <Instruction>. Try to be a harsh judge.\n",
    "\n",
    "<Instruction>\n",
    "{self.instruction}\n",
    "</Instruction>\n",
    "\n",
    "<Result>\n",
    "{run.outputs[\"output\"]}\n",
    "</Result>\n",
    "\"\"\"\n",
    "\n",
    "        structured_llm = self.eval_model.with_structured_output(\n",
    "            CorrectnessEvaluationResult)\n",
    "\n",
    "        result: CorrectnessEvaluationResult = structured_llm.invoke(\n",
    "            system_prompt)\n",
    "\n",
    "        return {\"score\": 1 if result.correct else 0, \"key\": \"correctness\", \"comment\": result.reasoning}\n",
    "\n",
    "    def _predict(self, inputs: dict):\n",
    "        \"\"\"Run current prompt against example in the dataset\"\"\"\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(content=self.current_prompt),\n",
    "                HumanMessage(content=inputs[\"input\"]),\n",
    "            ]\n",
    "\n",
    "            # Invoke the model\n",
    "            response = self.prediction_model.invoke(messages)\n",
    "            return {\"output\": response.content}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def _is_solved(self, eval_results):\n",
    "        \"\"\"Validate the results\"\"\"\n",
    "\n",
    "        results = eval_results._results\n",
    "\n",
    "        # If any of the result scores are not a 1, return false\n",
    "        if len(results) == 0:\n",
    "            raise Exception(\"No results found\")\n",
    "\n",
    "        for result in results:\n",
    "            score = result['evaluation_results'][\"results\"][0].score\n",
    "            if score != 1:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _test_rule(self, rule: Rule):\n",
    "        \"\"\"Evaluate a given rule\"\"\"\n",
    "        print(f'\\nTesting rule: \"{rule.prompt}\"')\n",
    "        self.current_rule = rule\n",
    "\n",
    "        self.current_prompt = self._build_prompt(self.current_rule)\n",
    "\n",
    "        results = evaluate(\n",
    "            self._predict,\n",
    "            data=self.dataset_name,\n",
    "            evaluators=[self._evaluate_correctness],\n",
    "            experiment_prefix=f\"Attempt-{self.attempts}\",\n",
    "        )\n",
    "\n",
    "        self.attempts += 1\n",
    "\n",
    "        return results\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the promptnado\"\"\"\n",
    "        print(f\"Running Promptnado with instruction: {self.instruction}\")\n",
    "        # Create the dataset\n",
    "        self._create_dataset()\n",
    "\n",
    "        while self.solved == False and self.attempts < self.max_attempts:\n",
    "            try:\n",
    "                self._generate_rules()\n",
    "                for rule in self.rules:\n",
    "                    if self.attempts > self.max_attempts:\n",
    "                        print(\"Max attempts reached\")\n",
    "                        return\n",
    "                    results = self._test_rule(rule)\n",
    "                    if self._is_solved(results):\n",
    "                        self.results = results\n",
    "                        self.solved = True\n",
    "                        self.successful_prompt = self.current_prompt\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Fatal error encountered: {e}\")\n",
    "                return  # Exit the while loop on error\n",
    "\n",
    "        print(\"\\n\\nSolved!! Current prompt can be found at `self.successful_prompt`\\n\\n\")\n",
    "\n",
    "        print(\n",
    "            f\"Successful prompt:\\n====================\\n{self.current_prompt}\\n=================\")\n",
    "        print(self.results._manager._experiment.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Promptnado with instruction: The agent should only respond in English. No other languages\n",
      "Created dataset: Promptnado_39970 with 2 examples\n",
      "Generated 5 rules\n",
      "\n",
      "[Rule(reasoning='By explicitly stating that all responses must be in English, we ensure the agent adheres to the language requirement.', prompt='You must respond only in English.'), Rule(reasoning='This prompt encourages the assistant to maintain English as the sole language for communication while discussing coding.', prompt='All coding discussions must be conducted in English only.'), Rule(reasoning='Stating that non-English responses are unacceptable reinforces the requirement for English-only communication.', prompt='Do not respond in any language other than English.'), Rule(reasoning='This prompt reminds the assistant to prioritize English in its responses, particularly in the context of coding.', prompt='Ensure all your replies are in English, focusing solely on coding topics.'), Rule(reasoning='By framing it as a directive, we make it clear that English is the required language for all interactions.', prompt='You are required to use English exclusively in your responses.')]\n",
      "\n",
      "Testing rule: \"You must respond only in English.\"\n",
      "Interpolated prompt:\n",
      "\n",
      "You are a helpful assistant\n",
      "\n",
      "Rules:\n",
      "- You are only allowed to talk about coding\n",
      "- You must respond only in English.\n",
      "- Try to be concise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camron/Documents/Dev/promptnado/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Attempt-1-1921a50e' at:\n",
      "https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/datasets/963dcbc4-9e9d-41ef-9797-f253634a893f/compare?selectedSessions=23fa3fe5-6491-4ef7-8e43-b94b0557e00d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solved!! Current prompt can be found at `self.successful_prompt`\n",
      "\n",
      "\n",
      "Successful prompt:\n",
      "====================\n",
      "You are a helpful assistant\n",
      "\n",
      "Rules:\n",
      "- You are only allowed to talk about coding\n",
      "- You must respond only in English.\n",
      "- Try to be concise\n",
      "=================\n",
      "https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/projects/p/23fa3fe5-6491-4ef7-8e43-b94b0557e00d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pn = Promptnado(example_system_prompt, example_instruction,\n",
    "                examples, max_attempts=2)\n",
    "pn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/projects/p/23fa3fe5-6491-4ef7-8e43-b94b0557e00d'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keys in results\n",
    "keys = pn.results._manager._experiment.url\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
