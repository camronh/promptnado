{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-anthropic langsmith python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_system_prompt = \"\"\"You are a helpful assistant\n",
    "\n",
    "Rules:\n",
    "- You are only allowed to talk about coding\n",
    "- <HERE>\n",
    "- Try to be concise\"\"\"\n",
    "\n",
    "example_instruction = \"The agent should only respond in English. No other languages\"\n",
    "\n",
    "examples = [\"Hola, como estas?\", \"Hi there!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith import evaluate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Schemas\n",
    "\n",
    "\n",
    "class Rule(BaseModel):\n",
    "    \"\"\"A single rule for the prompt\"\"\"\n",
    "    reasoning: str = Field(\n",
    "        ..., description=\"The thought process and direction for why we think this is a good solution to the instruction.\")\n",
    "    prompt: str = Field(..., description=\"A single prompt rule that we can try to solve for the instruction. \\\n",
    "This prompt rule will be interpolated into the system prompt over the <HERE> token.\")\n",
    "\n",
    "\n",
    "class Rules(BaseModel):\n",
    "    \"\"\"Set of prompt rules that should be tried\"\"\"\n",
    "    rules: List[Rule] = Field(\n",
    "        ..., description=\"A list of prompt rules that we can try to solve for the instruction.\")\n",
    "\n",
    "\n",
    "class Promptnado:\n",
    "    def __init__(self, system_prompt: str, instruction: str, examples: List[str], rule_token=\"<HERE>\"):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.instruction = instruction\n",
    "        self.examples = examples\n",
    "        self.rule_token = rule_token\n",
    "\n",
    "        # Create random dataset name\n",
    "        self.dataset_name = f\"Promptnado_{random.randint(0, 1000000)}\"\n",
    "\n",
    "        self.attempts = 1\n",
    "        self.solved = False\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        \"\"\"Create a dataset with a unique name\"\"\"\n",
    "        dataset = client.create_dataset(\n",
    "            self.dataset_name, description=self.instruction)\n",
    "        for example in self.examples:\n",
    "            client.create_example(\n",
    "                inputs={\"input\": example}, dataset_id=dataset.id)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        return dataset\n",
    "\n",
    "    def _generate_rules(self):\n",
    "        \"\"\"Use an LLM to generate a list of rules\"\"\"\n",
    "\n",
    "        system_prompt = f\"\"\"You are an expert LLM Prompt Engineer. Your job is to try to solve for the provided <Instructions> \\\n",
    "by making adjustments to the <Original Prompt>. You should attempt to make 5 suggestions for prompts that might work. Each suggestion you \\\n",
    "make will be interpolated into the prompt where {self.rule_token} is, and then evaluated for correctness against a dataset of \\\n",
    "examples.\n",
    "\n",
    "<Instructions>\n",
    "{self.instruction}\n",
    "</Instructions>\n",
    "\n",
    "<Original Prompt>\n",
    "{self.system_prompt}\n",
    "</Original Prompt>\n",
    "\"\"\"\n",
    "\n",
    "        structured_llm = init_chat_model(\n",
    "            model=\"gpt-4o\", temperature=0.7).with_structured_output(Rules)\n",
    "\n",
    "        rules: Rules = structured_llm.invoke(system_prompt)\n",
    "\n",
    "        self.rules = rules.rules\n",
    "        return self.rules\n",
    "\n",
    "    def _build_prompt(self, rule: Rule):\n",
    "        \"\"\"Interpolate the rules into the system prompt\"\"\"\n",
    "        interpolated_prompt = self.system_prompt.replace(\n",
    "            self.rule_token, rule.prompt)\n",
    "\n",
    "        return interpolated_prompt\n",
    "\n",
    "    def _evaluate_correctness(self, run: Run, example: Example):\n",
    "        \"\"\"Eval function to use an LLM to validate that the instruction was followed\"\"\"\n",
    "        return {\"score\": 0, \"key\": \"correctness\"}\n",
    "\n",
    "    def _predict(self, inputs: dict):\n",
    "        \"\"\"Predict the next rule\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.current_prompt),\n",
    "            HumanMessage(content=inputs[\"input\"]),\n",
    "        ]\n",
    "\n",
    "        # Invoke the model\n",
    "        llm = init_chat_model(model=\"gpt-4o\", temperature=0.7)\n",
    "        response = llm.invoke(messages)\n",
    "\n",
    "        return {\"output\": response.content}\n",
    "\n",
    "    def _test_rule(self, rule: Rule):\n",
    "        \"\"\"Evaluate a given rule\"\"\"\n",
    "        self.current_rule = rule\n",
    "\n",
    "        self.current_prompt = self._build_prompt(self.current_rule)\n",
    "\n",
    "        results = evaluate(\n",
    "            self._predict,\n",
    "            data=self.dataset_name,\n",
    "            evaluators=[self._evaluate_correctness],\n",
    "            experiment_prefix=f\"Attempt-{self.attempts}\",\n",
    "        )\n",
    "\n",
    "        self.attempts += 1\n",
    "\n",
    "        return results\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the promptnado\"\"\"\n",
    "        # Create the dataset\n",
    "        self._create_dataset()\n",
    "\n",
    "        while self.solved == False:\n",
    "            # Get a list of rules\n",
    "            self._generate_rules()\n",
    "\n",
    "            # For each rule\n",
    "            for rule in self.rules:\n",
    "                results = self._test_rule(rule)\n",
    "\n",
    "                # If the results are satisfactory, break\n",
    "                if results[\"correctness\"] > 0.9:\n",
    "                    self.solved = True\n",
    "                    break\n",
    "\n",
    "        print(\"\\n\\nSolved!! Current prompt can be found at `self.current_prompt\\n\\n\")\n",
    "\n",
    "        print(f\"Current prompt:\\n\\n{self.current_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
